{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 241600 entries, 0 to 241599\n",
      "Data columns (total 18 columns):\n",
      "MWG          241600 non-null int64\n",
      "NWG          241600 non-null int64\n",
      "KWG          241600 non-null int64\n",
      "MDIMC        241600 non-null int64\n",
      "NDIMC        241600 non-null int64\n",
      "MDIMA        241600 non-null int64\n",
      "NDIMB        241600 non-null int64\n",
      "KWI          241600 non-null int64\n",
      "VWM          241600 non-null int64\n",
      "VWN          241600 non-null int64\n",
      "STRM         241600 non-null int64\n",
      "STRN         241600 non-null int64\n",
      "SA           241600 non-null int64\n",
      "SB           241600 non-null int64\n",
      "Run1 (ms)    241600 non-null float64\n",
      "Run2 (ms)    241600 non-null float64\n",
      "Run3 (ms)    241600 non-null float64\n",
      "Run4 (ms)    241600 non-null float64\n",
      "dtypes: float64(4), int64(14)\n",
      "memory usage: 33.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "df1 = pd.read_csv('sgemm_product.csv')\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MWG          0\n",
       "NWG          0\n",
       "KWG          0\n",
       "MDIMC        0\n",
       "NDIMC        0\n",
       "MDIMA        0\n",
       "NDIMB        0\n",
       "KWI          0\n",
       "VWM          0\n",
       "VWN          0\n",
       "STRM         0\n",
       "STRN         0\n",
       "SA           0\n",
       "SB           0\n",
       "Run1 (ms)    0\n",
       "Run2 (ms)    0\n",
       "Run3 (ms)    0\n",
       "Run4 (ms)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for NA\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 241600 entries, 0 to 241599\n",
      "Data columns (total 19 columns):\n",
      "MWG                241600 non-null int64\n",
      "NWG                241600 non-null int64\n",
      "KWG                241600 non-null int64\n",
      "MDIMC              241600 non-null int64\n",
      "NDIMC              241600 non-null int64\n",
      "MDIMA              241600 non-null int64\n",
      "NDIMB              241600 non-null int64\n",
      "KWI                241600 non-null int64\n",
      "VWM                241600 non-null int64\n",
      "VWN                241600 non-null int64\n",
      "STRM               241600 non-null int64\n",
      "STRN               241600 non-null int64\n",
      "SA                 241600 non-null int64\n",
      "SB                 241600 non-null int64\n",
      "Run1 (ms)          241600 non-null float64\n",
      "Run2 (ms)          241600 non-null float64\n",
      "Run3 (ms)          241600 non-null float64\n",
      "Run4 (ms)          241600 non-null float64\n",
      "average_runtime    241600 non-null float64\n",
      "dtypes: float64(5), int64(14)\n",
      "memory usage: 35.0 MB\n"
     ]
    }
   ],
   "source": [
    "#Average runtime calculations\n",
    "df1['average_runtime']=df1[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MWG</th>\n",
       "      <th>NWG</th>\n",
       "      <th>KWG</th>\n",
       "      <th>MDIMC</th>\n",
       "      <th>NDIMC</th>\n",
       "      <th>MDIMA</th>\n",
       "      <th>NDIMB</th>\n",
       "      <th>KWI</th>\n",
       "      <th>VWM</th>\n",
       "      <th>VWN</th>\n",
       "      <th>STRM</th>\n",
       "      <th>STRN</th>\n",
       "      <th>SA</th>\n",
       "      <th>SB</th>\n",
       "      <th>average_runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116.3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>78.7050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80.5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>86.6375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118.6625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MWG  NWG  KWG  MDIMC  NDIMC  MDIMA  NDIMB  KWI  VWM  VWN  STRM  STRN  SA  \\\n",
       "0   16   16   16      8      8      8      8    2    1    1     0     0   0   \n",
       "1   16   16   16      8      8      8      8    2    1    1     0     0   0   \n",
       "2   16   16   16      8      8      8      8    2    1    1     0     0   1   \n",
       "3   16   16   16      8      8      8      8    2    1    1     0     0   1   \n",
       "4   16   16   16      8      8      8      8    2    1    1     0     1   0   \n",
       "\n",
       "   SB  average_runtime  \n",
       "0   0         116.3700  \n",
       "1   1          78.7050  \n",
       "2   0          80.5650  \n",
       "3   1          86.6375  \n",
       "4   0         118.6625  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df1.drop(['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'],axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f60001c308>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finding outliers using boxplot\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.boxplot(x='average_runtime', data = df1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier removal\n",
    "Q1=df1['average_runtime'].quantile(0.25)\n",
    "Q3=df1['average_runtime'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "LL=Q1-1.5*IQR\n",
    "UL=Q3+1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>MWG</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>75.688037</td>\n",
       "      <td>41.968313</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>64.00</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>128.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NWG</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>75.761619</td>\n",
       "      <td>41.997377</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>64.00</td>\n",
       "      <td>128.0000</td>\n",
       "      <td>128.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>KWG</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>25.592567</td>\n",
       "      <td>7.839899</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>32.00</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MDIMC</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>14.475690</td>\n",
       "      <td>8.111065</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NDIMC</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>14.470700</td>\n",
       "      <td>8.111264</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MDIMA</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>17.390550</td>\n",
       "      <td>9.375134</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NDIMB</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>17.388762</td>\n",
       "      <td>9.374445</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>16.00</td>\n",
       "      <td>32.0000</td>\n",
       "      <td>32.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>KWI</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>5.005544</td>\n",
       "      <td>3.000002</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VWM</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>2.339538</td>\n",
       "      <td>1.858623</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VWN</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>2.345156</td>\n",
       "      <td>1.862122</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>8.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STRM</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>0.500580</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>STRN</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SA</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>0.486038</td>\n",
       "      <td>0.499806</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SB</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>0.485056</td>\n",
       "      <td>0.499778</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>average_runtime</td>\n",
       "      <td>214833.0</td>\n",
       "      <td>114.554350</td>\n",
       "      <td>113.825481</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>39.095</td>\n",
       "      <td>61.79</td>\n",
       "      <td>157.8925</td>\n",
       "      <td>509.9625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count        mean         std      min     25%    50%  \\\n",
       "MWG              214833.0   75.688037   41.968313  16.0000  32.000  64.00   \n",
       "NWG              214833.0   75.761619   41.997377  16.0000  32.000  64.00   \n",
       "KWG              214833.0   25.592567    7.839899  16.0000  16.000  32.00   \n",
       "MDIMC            214833.0   14.475690    8.111065   8.0000   8.000  16.00   \n",
       "NDIMC            214833.0   14.470700    8.111264   8.0000   8.000  16.00   \n",
       "MDIMA            214833.0   17.390550    9.375134   8.0000   8.000  16.00   \n",
       "NDIMB            214833.0   17.388762    9.374445   8.0000   8.000  16.00   \n",
       "KWI              214833.0    5.005544    3.000002   2.0000   2.000   8.00   \n",
       "VWM              214833.0    2.339538    1.858623   1.0000   1.000   2.00   \n",
       "VWN              214833.0    2.345156    1.862122   1.0000   1.000   2.00   \n",
       "STRM             214833.0    0.500580    0.500001   0.0000   0.000   1.00   \n",
       "STRN             214833.0    0.500044    0.500001   0.0000   0.000   1.00   \n",
       "SA               214833.0    0.486038    0.499806   0.0000   0.000   0.00   \n",
       "SB               214833.0    0.485056    0.499778   0.0000   0.000   0.00   \n",
       "average_runtime  214833.0  114.554350  113.825481  13.3175  39.095  61.79   \n",
       "\n",
       "                      75%       max  \n",
       "MWG              128.0000  128.0000  \n",
       "NWG              128.0000  128.0000  \n",
       "KWG               32.0000   32.0000  \n",
       "MDIMC             16.0000   32.0000  \n",
       "NDIMC             16.0000   32.0000  \n",
       "MDIMA             32.0000   32.0000  \n",
       "NDIMB             32.0000   32.0000  \n",
       "KWI                8.0000    8.0000  \n",
       "VWM                4.0000    8.0000  \n",
       "VWN                4.0000    8.0000  \n",
       "STRM               1.0000    1.0000  \n",
       "STRN               1.0000    1.0000  \n",
       "SA                 1.0000    1.0000  \n",
       "SB                 1.0000    1.0000  \n",
       "average_runtime  157.8925  509.9625  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1[(df1.average_runtime>LL) & (df1.average_runtime<UL)]\n",
    "df2.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arush\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#converting average_runtime into 1 and 0 form for sv\n",
    "\n",
    "mean = df2['average_runtime'].mean()\n",
    "df2.loc[df2['average_runtime'] <= mean, 'average_runtime'] = 0\n",
    "df2.loc[df2['average_runtime'] > mean, 'average_runtime'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df2.iloc[:,1:14].values \n",
    "\n",
    "y1 = df2.iloc[:,14].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into test set and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105268 samples, validate on 45115 samples\n",
      "Epoch 1/200\n",
      "105268/105268 [==============================] - 22s 213us/step - loss: 0.5281 - accuracy: 0.7475 - val_loss: 0.5190 - val_accuracy: 0.7521\n",
      "Epoch 2/200\n",
      "105268/105268 [==============================] - 20s 195us/step - loss: 0.5130 - accuracy: 0.7563 - val_loss: 0.5158 - val_accuracy: 0.7558\n",
      "Epoch 3/200\n",
      "105268/105268 [==============================] - 21s 196us/step - loss: 0.5105 - accuracy: 0.7573 - val_loss: 0.5134 - val_accuracy: 0.7563\n",
      "Epoch 4/200\n",
      "105268/105268 [==============================] - 20s 192us/step - loss: 0.5077 - accuracy: 0.7579 - val_loss: 0.5083 - val_accuracy: 0.7507\n",
      "Epoch 5/200\n",
      "105268/105268 [==============================] - 20s 190us/step - loss: 0.4995 - accuracy: 0.7567 - val_loss: 0.5013 - val_accuracy: 0.7572\n",
      "Epoch 6/200\n",
      "105268/105268 [==============================] - 21s 201us/step - loss: 0.4965 - accuracy: 0.7570 - val_loss: 0.5005 - val_accuracy: 0.7509\n",
      "Epoch 7/200\n",
      "105268/105268 [==============================] - 20s 194us/step - loss: 0.4930 - accuracy: 0.7565 - val_loss: 0.4981 - val_accuracy: 0.7532\n",
      "Epoch 8/200\n",
      "105268/105268 [==============================] - 20s 194us/step - loss: 0.4920 - accuracy: 0.7550 - val_loss: 0.4959 - val_accuracy: 0.7510\n",
      "Epoch 9/200\n",
      "105268/105268 [==============================] - 21s 195us/step - loss: 0.4918 - accuracy: 0.7548 - val_loss: 0.4960 - val_accuracy: 0.7536\n",
      "Epoch 10/200\n",
      "105268/105268 [==============================] - 21s 196us/step - loss: 0.4914 - accuracy: 0.7543 - val_loss: 0.4963 - val_accuracy: 0.7509\n",
      "Epoch 11/200\n",
      "105268/105268 [==============================] - 20s 188us/step - loss: 0.4913 - accuracy: 0.7546 - val_loss: 0.4953 - val_accuracy: 0.7538\n",
      "Epoch 12/200\n",
      "105268/105268 [==============================] - 18s 176us/step - loss: 0.4907 - accuracy: 0.7536 - val_loss: 0.4990 - val_accuracy: 0.7562\n",
      "Epoch 13/200\n",
      "105268/105268 [==============================] - 18s 170us/step - loss: 0.4870 - accuracy: 0.7551 - val_loss: 0.4908 - val_accuracy: 0.7542\n",
      "Epoch 14/200\n",
      "105268/105268 [==============================] - 17s 166us/step - loss: 0.4858 - accuracy: 0.7573 - val_loss: 0.4917 - val_accuracy: 0.7566\n",
      "Epoch 15/200\n",
      "105268/105268 [==============================] - 17s 161us/step - loss: 0.4853 - accuracy: 0.7571 - val_loss: 0.4912 - val_accuracy: 0.7528\n",
      "Epoch 16/200\n",
      "105268/105268 [==============================] - 17s 163us/step - loss: 0.4849 - accuracy: 0.7574 - val_loss: 0.4927 - val_accuracy: 0.7559\n",
      "Epoch 17/200\n",
      "105268/105268 [==============================] - 17s 162us/step - loss: 0.4846 - accuracy: 0.7566 - val_loss: 0.4894 - val_accuracy: 0.7549\n",
      "Epoch 18/200\n",
      "105268/105268 [==============================] - 17s 163us/step - loss: 0.4844 - accuracy: 0.7584 - val_loss: 0.4899 - val_accuracy: 0.7551\n",
      "Epoch 19/200\n",
      "105268/105268 [==============================] - 17s 162us/step - loss: 0.4842 - accuracy: 0.7576 - val_loss: 0.4893 - val_accuracy: 0.7576\n",
      "Epoch 20/200\n",
      "105268/105268 [==============================] - 18s 170us/step - loss: 0.4842 - accuracy: 0.7578 - val_loss: 0.4883 - val_accuracy: 0.7588\n",
      "Epoch 21/200\n",
      "105268/105268 [==============================] - 18s 173us/step - loss: 0.4842 - accuracy: 0.7578 - val_loss: 0.4882 - val_accuracy: 0.7548\n",
      "Epoch 22/200\n",
      "105268/105268 [==============================] - 18s 171us/step - loss: 0.4839 - accuracy: 0.7578 - val_loss: 0.4892 - val_accuracy: 0.7568\n",
      "Epoch 23/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.4839 - accuracy: 0.7581 - val_loss: 0.4881 - val_accuracy: 0.7607\n",
      "Epoch 24/200\n",
      "105268/105268 [==============================] - 18s 173us/step - loss: 0.4835 - accuracy: 0.7584 - val_loss: 0.4879 - val_accuracy: 0.7577\n",
      "Epoch 25/200\n",
      "105268/105268 [==============================] - 18s 171us/step - loss: 0.4834 - accuracy: 0.7586 - val_loss: 0.4883 - val_accuracy: 0.7557\n",
      "Epoch 26/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.4835 - accuracy: 0.7585 - val_loss: 0.4879 - val_accuracy: 0.7534\n",
      "Epoch 27/200\n",
      "105268/105268 [==============================] - 18s 173us/step - loss: 0.4834 - accuracy: 0.7567 - val_loss: 0.4881 - val_accuracy: 0.7597\n",
      "Epoch 28/200\n",
      "105268/105268 [==============================] - 18s 173us/step - loss: 0.4832 - accuracy: 0.7578 - val_loss: 0.4887 - val_accuracy: 0.7588\n",
      "Epoch 29/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.4830 - accuracy: 0.7579 - val_loss: 0.4894 - val_accuracy: 0.7503\n",
      "Epoch 30/200\n",
      "105268/105268 [==============================] - 18s 169us/step - loss: 0.4829 - accuracy: 0.7583 - val_loss: 0.4902 - val_accuracy: 0.7578\n",
      "Epoch 31/200\n",
      "105268/105268 [==============================] - 18s 170us/step - loss: 0.4829 - accuracy: 0.7585 - val_loss: 0.4877 - val_accuracy: 0.7550\n",
      "Epoch 32/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.4828 - accuracy: 0.7579 - val_loss: 0.4881 - val_accuracy: 0.7565\n",
      "Epoch 33/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.4830 - accuracy: 0.7583 - val_loss: 0.4870 - val_accuracy: 0.7582\n",
      "Epoch 34/200\n",
      "105268/105268 [==============================] - 19s 184us/step - loss: 0.4827 - accuracy: 0.7575 - val_loss: 0.4885 - val_accuracy: 0.7513\n",
      "Epoch 35/200\n",
      "105268/105268 [==============================] - 18s 171us/step - loss: 0.4829 - accuracy: 0.7575 - val_loss: 0.4869 - val_accuracy: 0.7560\n",
      "Epoch 36/200\n",
      "105268/105268 [==============================] - 19s 180us/step - loss: 0.4825 - accuracy: 0.7580 - val_loss: 0.4867 - val_accuracy: 0.7535\n",
      "Epoch 37/200\n",
      "105268/105268 [==============================] - 19s 181us/step - loss: 0.4825 - accuracy: 0.7583 - val_loss: 0.4872 - val_accuracy: 0.7565\n",
      "Epoch 38/200\n",
      "105268/105268 [==============================] - 19s 179us/step - loss: 0.4823 - accuracy: 0.7580 - val_loss: 0.4871 - val_accuracy: 0.7572\n",
      "Epoch 39/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.4820 - accuracy: 0.7580 - val_loss: 0.4861 - val_accuracy: 0.7534\n",
      "Epoch 40/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.4820 - accuracy: 0.7572 - val_loss: 0.4867 - val_accuracy: 0.7538\n",
      "Epoch 41/200\n",
      "105268/105268 [==============================] - 19s 181us/step - loss: 0.4819 - accuracy: 0.7575 - val_loss: 0.4859 - val_accuracy: 0.7561\n",
      "Epoch 42/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.4818 - accuracy: 0.7568 - val_loss: 0.4879 - val_accuracy: 0.7603\n",
      "Epoch 43/200\n",
      "105268/105268 [==============================] - 19s 183us/step - loss: 0.4818 - accuracy: 0.7574 - val_loss: 0.4867 - val_accuracy: 0.7504\n",
      "Epoch 44/200\n",
      "105268/105268 [==============================] - 19s 183us/step - loss: 0.4816 - accuracy: 0.7574 - val_loss: 0.4867 - val_accuracy: 0.7567\n",
      "Epoch 45/200\n",
      "105268/105268 [==============================] - 19s 184us/step - loss: 0.4814 - accuracy: 0.7578 - val_loss: 0.4863 - val_accuracy: 0.7523\n",
      "Epoch 46/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.4804 - accuracy: 0.7571 - val_loss: 0.4847 - val_accuracy: 0.7607\n",
      "Epoch 47/200\n",
      "105268/105268 [==============================] - 19s 184us/step - loss: 0.4791 - accuracy: 0.7592 - val_loss: 0.4828 - val_accuracy: 0.7527\n",
      "Epoch 48/200\n",
      "105268/105268 [==============================] - 19s 183us/step - loss: 0.4748 - accuracy: 0.7599 - val_loss: 0.4753 - val_accuracy: 0.7583\n",
      "Epoch 49/200\n",
      "105268/105268 [==============================] - 19s 184us/step - loss: 0.4674 - accuracy: 0.7595 - val_loss: 0.4685 - val_accuracy: 0.7586\n",
      "Epoch 50/200\n",
      "105268/105268 [==============================] - 19s 184us/step - loss: 0.4515 - accuracy: 0.7680 - val_loss: 0.4462 - val_accuracy: 0.7717\n",
      "Epoch 51/200\n",
      "105268/105268 [==============================] - 19s 185us/step - loss: 0.4333 - accuracy: 0.7758 - val_loss: 0.4331 - val_accuracy: 0.7720\n",
      "Epoch 52/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.4252 - accuracy: 0.7792 - val_loss: 0.4246 - val_accuracy: 0.7825\n",
      "Epoch 53/200\n",
      "105268/105268 [==============================] - 20s 188us/step - loss: 0.4207 - accuracy: 0.7811 - val_loss: 0.4252 - val_accuracy: 0.7768\n",
      "Epoch 54/200\n",
      "105268/105268 [==============================] - 18s 174us/step - loss: 0.4185 - accuracy: 0.7798 - val_loss: 0.4213 - val_accuracy: 0.7808\n",
      "Epoch 55/200\n",
      "105268/105268 [==============================] - 20s 186us/step - loss: 0.4144 - accuracy: 0.7810 - val_loss: 0.4173 - val_accuracy: 0.7820\n",
      "Epoch 56/200\n",
      "105268/105268 [==============================] - 20s 186us/step - loss: 0.4097 - accuracy: 0.7824 - val_loss: 0.4125 - val_accuracy: 0.7775\n",
      "Epoch 57/200\n",
      "105268/105268 [==============================] - 29s 280us/step - loss: 0.4073 - accuracy: 0.7825 - val_loss: 0.4118 - val_accuracy: 0.7831\n",
      "Epoch 58/200\n",
      "105268/105268 [==============================] - 22s 210us/step - loss: 0.4061 - accuracy: 0.7845 - val_loss: 0.4124 - val_accuracy: 0.7778\n",
      "Epoch 59/200\n",
      "105268/105268 [==============================] - 17s 157us/step - loss: 0.4052 - accuracy: 0.7849 - val_loss: 0.4084 - val_accuracy: 0.7849\n",
      "Epoch 60/200\n",
      "105268/105268 [==============================] - 16s 156us/step - loss: 0.4048 - accuracy: 0.7850 - val_loss: 0.4077 - val_accuracy: 0.7841\n",
      "Epoch 61/200\n",
      "105268/105268 [==============================] - 16s 155us/step - loss: 0.4044 - accuracy: 0.7845 - val_loss: 0.4087 - val_accuracy: 0.7820\n",
      "Epoch 62/200\n",
      "105268/105268 [==============================] - 16s 154us/step - loss: 0.4038 - accuracy: 0.7853 - val_loss: 0.4094 - val_accuracy: 0.7824\n",
      "Epoch 63/200\n",
      "105268/105268 [==============================] - 16s 155us/step - loss: 0.4031 - accuracy: 0.7865 - val_loss: 0.4064 - val_accuracy: 0.7865\n",
      "Epoch 64/200\n",
      "105268/105268 [==============================] - 17s 157us/step - loss: 0.4027 - accuracy: 0.7861 - val_loss: 0.4051 - val_accuracy: 0.7840\n",
      "Epoch 65/200\n",
      "105268/105268 [==============================] - 16s 157us/step - loss: 0.4024 - accuracy: 0.7869 - val_loss: 0.4058 - val_accuracy: 0.7845\n",
      "Epoch 66/200\n",
      "105268/105268 [==============================] - 16s 157us/step - loss: 0.4017 - accuracy: 0.7868 - val_loss: 0.4055 - val_accuracy: 0.7849\n",
      "Epoch 67/200\n",
      "105268/105268 [==============================] - 18s 171us/step - loss: 0.4013 - accuracy: 0.7866 - val_loss: 0.4067 - val_accuracy: 0.7837\n",
      "Epoch 68/200\n",
      "105268/105268 [==============================] - 17s 164us/step - loss: 0.4012 - accuracy: 0.7874 - val_loss: 0.4090 - val_accuracy: 0.7803\n",
      "Epoch 69/200\n",
      "105268/105268 [==============================] - 22s 210us/step - loss: 0.4006 - accuracy: 0.7876 - val_loss: 0.4043 - val_accuracy: 0.7825\n",
      "Epoch 70/200\n",
      "105268/105268 [==============================] - 21s 203us/step - loss: 0.4008 - accuracy: 0.7863 - val_loss: 0.4069 - val_accuracy: 0.7792\n",
      "Epoch 71/200\n",
      "105268/105268 [==============================] - 18s 173us/step - loss: 0.4008 - accuracy: 0.7879 - val_loss: 0.4041 - val_accuracy: 0.7845\n",
      "Epoch 72/200\n",
      "105268/105268 [==============================] - 24s 226us/step - loss: 0.4011 - accuracy: 0.7872 - val_loss: 0.4051 - val_accuracy: 0.7841\n",
      "Epoch 73/200\n",
      "105268/105268 [==============================] - 22s 208us/step - loss: 0.4010 - accuracy: 0.7874 - val_loss: 0.4046 - val_accuracy: 0.7859\n",
      "Epoch 74/200\n",
      "105268/105268 [==============================] - 20s 185us/step - loss: 0.4008 - accuracy: 0.7870 - val_loss: 0.4065 - val_accuracy: 0.7861\n",
      "Epoch 75/200\n",
      "105268/105268 [==============================] - 20s 195us/step - loss: 0.3990 - accuracy: 0.7877 - val_loss: 0.4059 - val_accuracy: 0.7769\n",
      "Epoch 76/200\n",
      "105268/105268 [==============================] - 18s 168us/step - loss: 0.3959 - accuracy: 0.7882 - val_loss: 0.4002 - val_accuracy: 0.7846\n",
      "Epoch 77/200\n",
      "105268/105268 [==============================] - 18s 168us/step - loss: 0.3953 - accuracy: 0.7880 - val_loss: 0.3999 - val_accuracy: 0.7879\n",
      "Epoch 78/200\n",
      "105268/105268 [==============================] - 19s 177us/step - loss: 0.3946 - accuracy: 0.7889 - val_loss: 0.3996 - val_accuracy: 0.7847\n",
      "Epoch 79/200\n",
      "105268/105268 [==============================] - 20s 188us/step - loss: 0.3940 - accuracy: 0.7899 - val_loss: 0.3987 - val_accuracy: 0.7853\n",
      "Epoch 80/200\n",
      "105268/105268 [==============================] - 20s 194us/step - loss: 0.3944 - accuracy: 0.7884 - val_loss: 0.3975 - val_accuracy: 0.7856\n",
      "Epoch 81/200\n",
      "105268/105268 [==============================] - 25s 235us/step - loss: 0.3940 - accuracy: 0.7895 - val_loss: 0.4005 - val_accuracy: 0.7876\n",
      "Epoch 82/200\n",
      "105268/105268 [==============================] - 23s 216us/step - loss: 0.3943 - accuracy: 0.7890 - val_loss: 0.3987 - val_accuracy: 0.7860\n",
      "Epoch 83/200\n",
      "105268/105268 [==============================] - 23s 220us/step - loss: 0.3939 - accuracy: 0.7890 - val_loss: 0.3993 - val_accuracy: 0.7872\n",
      "Epoch 84/200\n",
      "105268/105268 [==============================] - 23s 223us/step - loss: 0.3939 - accuracy: 0.7890 - val_loss: 0.3977 - val_accuracy: 0.7890\n",
      "Epoch 85/200\n",
      "105268/105268 [==============================] - 23s 223us/step - loss: 0.3942 - accuracy: 0.7893 - val_loss: 0.4017 - val_accuracy: 0.7863\n",
      "Epoch 86/200\n",
      "105268/105268 [==============================] - 23s 215us/step - loss: 0.3939 - accuracy: 0.7893 - val_loss: 0.4050 - val_accuracy: 0.7853\n",
      "Epoch 87/200\n",
      "105268/105268 [==============================] - 23s 220us/step - loss: 0.3942 - accuracy: 0.7898 - val_loss: 0.4007 - val_accuracy: 0.7868\n",
      "Epoch 88/200\n",
      "105268/105268 [==============================] - 23s 222us/step - loss: 0.3940 - accuracy: 0.7891 - val_loss: 0.3993 - val_accuracy: 0.7902\n",
      "Epoch 89/200\n",
      "105268/105268 [==============================] - 24s 226us/step - loss: 0.3938 - accuracy: 0.7897 - val_loss: 0.3979 - val_accuracy: 0.7880\n",
      "Epoch 90/200\n",
      "105268/105268 [==============================] - 23s 223us/step - loss: 0.3940 - accuracy: 0.7891 - val_loss: 0.3993 - val_accuracy: 0.7885\n",
      "Epoch 91/200\n",
      "105268/105268 [==============================] - 24s 228us/step - loss: 0.3938 - accuracy: 0.7892 - val_loss: 0.3971 - val_accuracy: 0.7879\n",
      "Epoch 92/200\n",
      "105268/105268 [==============================] - 24s 228us/step - loss: 0.3939 - accuracy: 0.7900 - val_loss: 0.4009 - val_accuracy: 0.7863\n",
      "Epoch 93/200\n",
      "105268/105268 [==============================] - 23s 219us/step - loss: 0.3939 - accuracy: 0.7889 - val_loss: 0.4025 - val_accuracy: 0.7823\n",
      "Epoch 94/200\n",
      "105268/105268 [==============================] - 24s 223us/step - loss: 0.3944 - accuracy: 0.7891 - val_loss: 0.3988 - val_accuracy: 0.7851\n",
      "Epoch 95/200\n",
      "105268/105268 [==============================] - 25s 238us/step - loss: 0.3938 - accuracy: 0.7887 - val_loss: 0.3994 - val_accuracy: 0.7868\n",
      "Epoch 96/200\n",
      "105268/105268 [==============================] - 23s 220us/step - loss: 0.3939 - accuracy: 0.7891 - val_loss: 0.4018 - val_accuracy: 0.7869\n",
      "Epoch 97/200\n",
      "105268/105268 [==============================] - 23s 223us/step - loss: 0.3940 - accuracy: 0.7881 - val_loss: 0.3995 - val_accuracy: 0.7874\n",
      "Epoch 98/200\n",
      "105268/105268 [==============================] - 23s 220us/step - loss: 0.3939 - accuracy: 0.7886 - val_loss: 0.3994 - val_accuracy: 0.7894\n",
      "Epoch 99/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3937 - accuracy: 0.7889 - val_loss: 0.4014 - val_accuracy: 0.7863\n",
      "Epoch 100/200\n",
      "105268/105268 [==============================] - 23s 220us/step - loss: 0.3940 - accuracy: 0.7885 - val_loss: 0.4016 - val_accuracy: 0.7873\n",
      "Epoch 101/200\n",
      "105268/105268 [==============================] - 24s 227us/step - loss: 0.3937 - accuracy: 0.7885 - val_loss: 0.3991 - val_accuracy: 0.7824\n",
      "Epoch 102/200\n",
      "105268/105268 [==============================] - 25s 240us/step - loss: 0.3942 - accuracy: 0.7889 - val_loss: 0.4014 - val_accuracy: 0.7840\n",
      "Epoch 103/200\n",
      "105268/105268 [==============================] - 24s 231us/step - loss: 0.3938 - accuracy: 0.7888 - val_loss: 0.4034 - val_accuracy: 0.7808\n",
      "Epoch 104/200\n",
      "105268/105268 [==============================] - 24s 232us/step - loss: 0.3938 - accuracy: 0.7883 - val_loss: 0.3988 - val_accuracy: 0.7869\n",
      "Epoch 105/200\n",
      "105268/105268 [==============================] - 24s 229us/step - loss: 0.3936 - accuracy: 0.7880 - val_loss: 0.4045 - val_accuracy: 0.7847\n",
      "Epoch 106/200\n",
      "105268/105268 [==============================] - 24s 229us/step - loss: 0.3939 - accuracy: 0.7886 - val_loss: 0.4026 - val_accuracy: 0.7878\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105268/105268 [==============================] - 24s 224us/step - loss: 0.3936 - accuracy: 0.7891 - val_loss: 0.4012 - val_accuracy: 0.7878\n",
      "Epoch 108/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3935 - accuracy: 0.7890 - val_loss: 0.3973 - val_accuracy: 0.7858\n",
      "Epoch 109/200\n",
      "105268/105268 [==============================] - 23s 214us/step - loss: 0.3937 - accuracy: 0.7887 - val_loss: 0.3981 - val_accuracy: 0.7823\n",
      "Epoch 110/200\n",
      "105268/105268 [==============================] - 24s 224us/step - loss: 0.3934 - accuracy: 0.7895 - val_loss: 0.3988 - val_accuracy: 0.7865\n",
      "Epoch 111/200\n",
      "105268/105268 [==============================] - 24s 226us/step - loss: 0.3935 - accuracy: 0.7887 - val_loss: 0.3986 - val_accuracy: 0.7868\n",
      "Epoch 112/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3934 - accuracy: 0.7880 - val_loss: 0.3981 - val_accuracy: 0.7851\n",
      "Epoch 113/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3936 - accuracy: 0.7885 - val_loss: 0.4008 - val_accuracy: 0.7871\n",
      "Epoch 114/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3934 - accuracy: 0.7885 - val_loss: 0.3977 - val_accuracy: 0.7873\n",
      "Epoch 115/200\n",
      "105268/105268 [==============================] - 24s 225us/step - loss: 0.3928 - accuracy: 0.7889 - val_loss: 0.3974 - val_accuracy: 0.7867\n",
      "Epoch 116/200\n",
      "105268/105268 [==============================] - 24s 230us/step - loss: 0.3930 - accuracy: 0.7880 - val_loss: 0.3988 - val_accuracy: 0.7842\n",
      "Epoch 117/200\n",
      "105268/105268 [==============================] - 23s 221us/step - loss: 0.3930 - accuracy: 0.7892 - val_loss: 0.3967 - val_accuracy: 0.7878\n",
      "Epoch 118/200\n",
      "105268/105268 [==============================] - 19s 178us/step - loss: 0.3928 - accuracy: 0.7888 - val_loss: 0.3973 - val_accuracy: 0.7885\n",
      "Epoch 119/200\n",
      "105268/105268 [==============================] - 18s 175us/step - loss: 0.3925 - accuracy: 0.7891 - val_loss: 0.3985 - val_accuracy: 0.7864\n",
      "Epoch 120/200\n",
      "105268/105268 [==============================] - 17s 164us/step - loss: 0.3924 - accuracy: 0.7886 - val_loss: 0.3966 - val_accuracy: 0.7855\n",
      "Epoch 121/200\n",
      "105268/105268 [==============================] - 17s 162us/step - loss: 0.3919 - accuracy: 0.7881 - val_loss: 0.3968 - val_accuracy: 0.7906\n",
      "Epoch 122/200\n",
      "105268/105268 [==============================] - 17s 164us/step - loss: 0.3919 - accuracy: 0.7884 - val_loss: 0.3953 - val_accuracy: 0.7865\n",
      "Epoch 123/200\n",
      "105268/105268 [==============================] - 17s 163us/step - loss: 0.3919 - accuracy: 0.7886 - val_loss: 0.3950 - val_accuracy: 0.7897\n",
      "Epoch 124/200\n",
      "105268/105268 [==============================] - 17s 161us/step - loss: 0.3915 - accuracy: 0.7885 - val_loss: 0.3962 - val_accuracy: 0.7899\n",
      "Epoch 125/200\n",
      "105268/105268 [==============================] - 19s 180us/step - loss: 0.3914 - accuracy: 0.7886 - val_loss: 0.3951 - val_accuracy: 0.7871\n",
      "Epoch 126/200\n",
      "105268/105268 [==============================] - 19s 183us/step - loss: 0.3915 - accuracy: 0.7879 - val_loss: 0.3965 - val_accuracy: 0.7906\n",
      "Epoch 127/200\n",
      "105268/105268 [==============================] - 19s 182us/step - loss: 0.3912 - accuracy: 0.7891 - val_loss: 0.3952 - val_accuracy: 0.7830\n",
      "Epoch 128/200\n",
      "105268/105268 [==============================] - 20s 194us/step - loss: 0.3909 - accuracy: 0.7890 - val_loss: 0.3970 - val_accuracy: 0.7873\n",
      "Epoch 129/200\n",
      "105268/105268 [==============================] - 22s 207us/step - loss: 0.3911 - accuracy: 0.7882 - val_loss: 0.3957 - val_accuracy: 0.7874\n",
      "Epoch 130/200\n",
      "105268/105268 [==============================] - 24s 226us/step - loss: 0.3910 - accuracy: 0.7886 - val_loss: 0.3964 - val_accuracy: 0.7883\n",
      "Epoch 131/200\n",
      "105268/105268 [==============================] - 26s 247us/step - loss: 0.3912 - accuracy: 0.7887 - val_loss: 0.3955 - val_accuracy: 0.7838\n",
      "Epoch 132/200\n",
      "105268/105268 [==============================] - 24s 233us/step - loss: 0.3909 - accuracy: 0.7887 - val_loss: 0.3955 - val_accuracy: 0.7872\n",
      "Epoch 133/200\n",
      "105268/105268 [==============================] - 25s 237us/step - loss: 0.3908 - accuracy: 0.7882 - val_loss: 0.3958 - val_accuracy: 0.7848\n",
      "Epoch 134/200\n",
      "105268/105268 [==============================] - 25s 237us/step - loss: 0.3907 - accuracy: 0.7879 - val_loss: 0.3992 - val_accuracy: 0.7884\n",
      "Epoch 135/200\n",
      "105268/105268 [==============================] - 25s 237us/step - loss: 0.3911 - accuracy: 0.7890 - val_loss: 0.3947 - val_accuracy: 0.7874\n",
      "Epoch 136/200\n",
      "105268/105268 [==============================] - 26s 243us/step - loss: 0.3909 - accuracy: 0.7876 - val_loss: 0.3979 - val_accuracy: 0.7872\n",
      "Epoch 137/200\n",
      "105268/105268 [==============================] - 26s 246us/step - loss: 0.3909 - accuracy: 0.7892 - val_loss: 0.3950 - val_accuracy: 0.7882\n",
      "Epoch 138/200\n",
      "105268/105268 [==============================] - 25s 239us/step - loss: 0.3908 - accuracy: 0.7884 - val_loss: 0.3984 - val_accuracy: 0.7843\n",
      "Epoch 139/200\n",
      "105268/105268 [==============================] - 28s 265us/step - loss: 0.3911 - accuracy: 0.7884 - val_loss: 0.3944 - val_accuracy: 0.7882\n",
      "Epoch 140/200\n",
      "105268/105268 [==============================] - 33s 311us/step - loss: 0.3910 - accuracy: 0.7881 - val_loss: 0.3962 - val_accuracy: 0.7875\n",
      "Epoch 141/200\n",
      "105268/105268 [==============================] - 26s 243us/step - loss: 0.3906 - accuracy: 0.7894 - val_loss: 0.3952 - val_accuracy: 0.7899\n",
      "Epoch 142/200\n",
      "105268/105268 [==============================] - 24s 232us/step - loss: 0.3908 - accuracy: 0.7887 - val_loss: 0.3961 - val_accuracy: 0.7842\n",
      "Epoch 143/200\n",
      "105268/105268 [==============================] - 26s 243us/step - loss: 0.3908 - accuracy: 0.7883 - val_loss: 0.3943 - val_accuracy: 0.7894\n",
      "Epoch 144/200\n",
      "105268/105268 [==============================] - 24s 227us/step - loss: 0.3907 - accuracy: 0.7887 - val_loss: 0.3947 - val_accuracy: 0.7886\n",
      "Epoch 145/200\n",
      "105268/105268 [==============================] - 23s 222us/step - loss: 0.3907 - accuracy: 0.7897 - val_loss: 0.3964 - val_accuracy: 0.7853\n",
      "Epoch 146/200\n",
      "105268/105268 [==============================] - 25s 234us/step - loss: 0.3910 - accuracy: 0.7889 - val_loss: 0.3950 - val_accuracy: 0.7902\n",
      "Epoch 147/200\n",
      "105268/105268 [==============================] - 24s 224us/step - loss: 0.3907 - accuracy: 0.7880 - val_loss: 0.3939 - val_accuracy: 0.7866\n",
      "Epoch 148/200\n",
      "105268/105268 [==============================] - 25s 239us/step - loss: 0.3910 - accuracy: 0.7884 - val_loss: 0.3955 - val_accuracy: 0.7841\n",
      "Epoch 149/200\n",
      "105268/105268 [==============================] - 25s 235us/step - loss: 0.3907 - accuracy: 0.7886 - val_loss: 0.3940 - val_accuracy: 0.7900\n",
      "Epoch 150/200\n",
      "105268/105268 [==============================] - 24s 230us/step - loss: 0.3905 - accuracy: 0.7891 - val_loss: 0.3989 - val_accuracy: 0.7861\n",
      "Epoch 151/200\n",
      "105268/105268 [==============================] - 24s 231us/step - loss: 0.3908 - accuracy: 0.7889 - val_loss: 0.3960 - val_accuracy: 0.7844\n",
      "Epoch 152/200\n",
      "105268/105268 [==============================] - 25s 235us/step - loss: 0.3907 - accuracy: 0.7880 - val_loss: 0.3958 - val_accuracy: 0.7857\n",
      "Epoch 153/200\n",
      "105268/105268 [==============================] - 24s 230us/step - loss: 0.3903 - accuracy: 0.7896 - val_loss: 0.3974 - val_accuracy: 0.7831\n",
      "Epoch 154/200\n",
      "105268/105268 [==============================] - 25s 237us/step - loss: 0.3906 - accuracy: 0.7896 - val_loss: 0.3984 - val_accuracy: 0.7845\n",
      "Epoch 155/200\n",
      "105268/105268 [==============================] - 26s 250us/step - loss: 0.3907 - accuracy: 0.7884 - val_loss: 0.3973 - val_accuracy: 0.7890\n",
      "Epoch 156/200\n",
      "105268/105268 [==============================] - 26s 243us/step - loss: 0.3908 - accuracy: 0.7888 - val_loss: 0.3961 - val_accuracy: 0.7871\n",
      "Epoch 157/200\n",
      "105268/105268 [==============================] - 22s 209us/step - loss: 0.3909 - accuracy: 0.7888 - val_loss: 0.3991 - val_accuracy: 0.7858\n",
      "Epoch 158/200\n",
      "105268/105268 [==============================] - 19s 185us/step - loss: 0.3907 - accuracy: 0.7888 - val_loss: 0.3950 - val_accuracy: 0.7878\n",
      "Epoch 159/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.3908 - accuracy: 0.7900 - val_loss: 0.3970 - val_accuracy: 0.7864\n",
      "Epoch 160/200\n",
      "105268/105268 [==============================] - 19s 183us/step - loss: 0.3908 - accuracy: 0.7892 - val_loss: 0.3973 - val_accuracy: 0.7877\n",
      "Epoch 161/200\n",
      "105268/105268 [==============================] - 18s 175us/step - loss: 0.3904 - accuracy: 0.7886 - val_loss: 0.3996 - val_accuracy: 0.7843\n",
      "Epoch 162/200\n",
      "105268/105268 [==============================] - 17s 163us/step - loss: 0.3907 - accuracy: 0.7885 - val_loss: 0.3968 - val_accuracy: 0.7843\n",
      "Epoch 163/200\n",
      "105268/105268 [==============================] - 17s 157us/step - loss: 0.3905 - accuracy: 0.7894 - val_loss: 0.3959 - val_accuracy: 0.7830\n",
      "Epoch 164/200\n",
      "105268/105268 [==============================] - 20s 191us/step - loss: 0.3904 - accuracy: 0.7890 - val_loss: 0.3969 - val_accuracy: 0.7842\n",
      "Epoch 165/200\n",
      "105268/105268 [==============================] - 15s 140us/step - loss: 0.3907 - accuracy: 0.7890 - val_loss: 0.4047 - val_accuracy: 0.7829\n",
      "Epoch 166/200\n",
      "105268/105268 [==============================] - 15s 139us/step - loss: 0.3908 - accuracy: 0.7887 - val_loss: 0.3951 - val_accuracy: 0.7872\n",
      "Epoch 167/200\n",
      "105268/105268 [==============================] - 15s 140us/step - loss: 0.3907 - accuracy: 0.7880 - val_loss: 0.3973 - val_accuracy: 0.7851\n",
      "Epoch 168/200\n",
      "105268/105268 [==============================] - 17s 161us/step - loss: 0.3907 - accuracy: 0.7881 - val_loss: 0.3962 - val_accuracy: 0.7840\n",
      "Epoch 169/200\n",
      "105268/105268 [==============================] - 20s 190us/step - loss: 0.3908 - accuracy: 0.7885 - val_loss: 0.3957 - val_accuracy: 0.7858\n",
      "Epoch 170/200\n",
      "105268/105268 [==============================] - 15s 144us/step - loss: 0.3905 - accuracy: 0.7889 - val_loss: 0.3976 - val_accuracy: 0.7837\n",
      "Epoch 171/200\n",
      "105268/105268 [==============================] - 16s 148us/step - loss: 0.3907 - accuracy: 0.7887 - val_loss: 0.3956 - val_accuracy: 0.7842\n",
      "Epoch 172/200\n",
      "105268/105268 [==============================] - 16s 152us/step - loss: 0.3908 - accuracy: 0.7888 - val_loss: 0.3956 - val_accuracy: 0.7889\n",
      "Epoch 173/200\n",
      "105268/105268 [==============================] - 15s 141us/step - loss: 0.3908 - accuracy: 0.7891 - val_loss: 0.3951 - val_accuracy: 0.7869\n",
      "Epoch 174/200\n",
      "105268/105268 [==============================] - 15s 146us/step - loss: 0.3905 - accuracy: 0.7888 - val_loss: 0.3944 - val_accuracy: 0.7856\n",
      "Epoch 175/200\n",
      "105268/105268 [==============================] - 16s 149us/step - loss: 0.3906 - accuracy: 0.7887 - val_loss: 0.3991 - val_accuracy: 0.7851\n",
      "Epoch 176/200\n",
      "105268/105268 [==============================] - 17s 165us/step - loss: 0.3905 - accuracy: 0.7881 - val_loss: 0.3946 - val_accuracy: 0.7892\n",
      "Epoch 177/200\n",
      "105268/105268 [==============================] - 17s 159us/step - loss: 0.3906 - accuracy: 0.7900 - val_loss: 0.3965 - val_accuracy: 0.7856\n",
      "Epoch 178/200\n",
      "105268/105268 [==============================] - 18s 169us/step - loss: 0.3903 - accuracy: 0.7892 - val_loss: 0.3951 - val_accuracy: 0.7875\n",
      "Epoch 179/200\n",
      "105268/105268 [==============================] - 18s 167us/step - loss: 0.3902 - accuracy: 0.7896 - val_loss: 0.3949 - val_accuracy: 0.7861\n",
      "Epoch 180/200\n",
      "105268/105268 [==============================] - 19s 180us/step - loss: 0.3906 - accuracy: 0.7881 - val_loss: 0.3940 - val_accuracy: 0.7860\n",
      "Epoch 181/200\n",
      "105268/105268 [==============================] - 18s 172us/step - loss: 0.3907 - accuracy: 0.7888 - val_loss: 0.3955 - val_accuracy: 0.7858\n",
      "Epoch 182/200\n",
      "105268/105268 [==============================] - 18s 176us/step - loss: 0.3906 - accuracy: 0.7887 - val_loss: 0.3950 - val_accuracy: 0.7884\n",
      "Epoch 183/200\n",
      "105268/105268 [==============================] - 23s 216us/step - loss: 0.3903 - accuracy: 0.7892 - val_loss: 0.3961 - val_accuracy: 0.7893\n",
      "Epoch 184/200\n",
      "105268/105268 [==============================] - 27s 256us/step - loss: 0.3907 - accuracy: 0.7887 - val_loss: 0.4028 - val_accuracy: 0.7816\n",
      "Epoch 185/200\n",
      "105268/105268 [==============================] - 28s 267us/step - loss: 0.3904 - accuracy: 0.7887 - val_loss: 0.3950 - val_accuracy: 0.7896\n",
      "Epoch 186/200\n",
      "105268/105268 [==============================] - 28s 262us/step - loss: 0.3906 - accuracy: 0.7885 - val_loss: 0.3953 - val_accuracy: 0.7910\n",
      "Epoch 187/200\n",
      "105268/105268 [==============================] - 27s 252us/step - loss: 0.3910 - accuracy: 0.7888 - val_loss: 0.3973 - val_accuracy: 0.7858\n",
      "Epoch 188/200\n",
      "105268/105268 [==============================] - 26s 251us/step - loss: 0.3905 - accuracy: 0.7883 - val_loss: 0.3956 - val_accuracy: 0.7891\n",
      "Epoch 189/200\n",
      "105268/105268 [==============================] - 26s 249us/step - loss: 0.3906 - accuracy: 0.7894 - val_loss: 0.4053 - val_accuracy: 0.7826\n",
      "Epoch 190/200\n",
      "105268/105268 [==============================] - 27s 254us/step - loss: 0.3905 - accuracy: 0.7883 - val_loss: 0.3960 - val_accuracy: 0.7884\n",
      "Epoch 191/200\n",
      "105268/105268 [==============================] - 29s 276us/step - loss: 0.3907 - accuracy: 0.7891 - val_loss: 0.3951 - val_accuracy: 0.7890\n",
      "Epoch 192/200\n",
      "105268/105268 [==============================] - 27s 255us/step - loss: 0.3900 - accuracy: 0.7896 - val_loss: 0.3944 - val_accuracy: 0.7882\n",
      "Epoch 193/200\n",
      "105268/105268 [==============================] - 28s 270us/step - loss: 0.3901 - accuracy: 0.7896 - val_loss: 0.3983 - val_accuracy: 0.7836\n",
      "Epoch 194/200\n",
      "105268/105268 [==============================] - 26s 247us/step - loss: 0.3904 - accuracy: 0.7897 - val_loss: 0.3982 - val_accuracy: 0.7869\n",
      "Epoch 195/200\n",
      "105268/105268 [==============================] - 26s 244us/step - loss: 0.3906 - accuracy: 0.7888 - val_loss: 0.3959 - val_accuracy: 0.7886\n",
      "Epoch 196/200\n",
      "105268/105268 [==============================] - 26s 247us/step - loss: 0.3906 - accuracy: 0.7887 - val_loss: 0.3976 - val_accuracy: 0.7863\n",
      "Epoch 197/200\n",
      "105268/105268 [==============================] - 25s 242us/step - loss: 0.3904 - accuracy: 0.7886 - val_loss: 0.3942 - val_accuracy: 0.7865\n",
      "Epoch 198/200\n",
      "105268/105268 [==============================] - 27s 254us/step - loss: 0.3902 - accuracy: 0.7890 - val_loss: 0.3968 - val_accuracy: 0.7874\n",
      "Epoch 199/200\n",
      "105268/105268 [==============================] - 24s 231us/step - loss: 0.3905 - accuracy: 0.7891 - val_loss: 0.3943 - val_accuracy: 0.7856\n",
      "Epoch 200/200\n",
      "105268/105268 [==============================] - 26s 242us/step - loss: 0.3903 - accuracy: 0.7884 - val_loss: 0.4049 - val_accuracy: 0.7813\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=13, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 200,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39768  3416]\n",
      " [10430 10836]] \n",
      "\n",
      "The accuracy is 0.785166795965865\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6f530bc33081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Initialising the ANN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Adding the input layer and the first hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "#100\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=13, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fe56a6b06b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Predicting the Test set results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#y_pred = (y_pred > 0.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Making the Confusion Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Now let's make the ANN!\n",
    "\n",
    "#400\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=13, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 400,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 (For different layers)\n",
    "#no of layers=3\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=89, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#no of layers=4\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=26, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#number of layers=5\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=26, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X1_train, y1_train, batch_size = 10, epochs = 100,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "# no of layers=10\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=26, units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the sixth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "# Adding the seventh hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "# Adding the eighth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "# Adding the ninth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "# Adding the tenth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=14, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#sigmoid\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", input_dim=89, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#softmax\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"softmax\", input_dim=89, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"softmax\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"softmax\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"softmax\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"softmax\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "#tanh\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", input_dim=89, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the number of nodes:\n",
    "#changing the number of nodes to 16.\n",
    "\n",
    "# Initialising the ANN\n",
    "#number of layers=5\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=89, units=6, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fourth hidden layer\n",
    "classifier.add(Dense(activation=\"tanh\", units=16, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Adding the fifth hidden layer\n",
    "classifier.add(Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\"))\n",
    "\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "history=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict_classes(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accu=accuracy_score(y_test,y_pred)\n",
    "print(cm,\"\\n\")\n",
    "print(\"The accuracy is\",accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN FOR DATASET 2\n",
    "##### Experiment 1 with k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=5\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 5 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 5 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 10 folds\\n\",std*100,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 (Euclidean distance) with k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=3\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 5 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 5 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 5 folds\\n\",std*100,end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search for k with euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing around with the KNN with n parameter to get the best n\n",
    "#performing grid search to find the best value of n in KNN and the most efficient values for Hyperparameters\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "# Fitting a general classifier to use in Grid Search to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(metric = 'minkowski')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_neighbors': range(2,40),'p':[2]}] \n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(best_accuracy,end='\\n')\n",
    "print(best_parameters,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i,p=2,metric='minkowski')\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 40), error, color='black', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='grey', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>KNN with the Best values reported from Grid Serach for Euclidian Distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 38, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=38\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 10 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 5 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 5 folds\\n\",std*100,end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn with manhatten distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=5\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 10 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 5 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 5 folds\\n\",std*100,end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second experiment with k = 3(manhatten distance)\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=3\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 10 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 10 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 10 folds\\n\",std*100,end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for k with manhatten distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing around with the KNN with n parameter to get the best n\n",
    "#performing grid search to find the best value of n in KNN and the most efficient values for Hyperparameters\n",
    "# Applying Grid Search to find the best model and the best parameters\n",
    "# Fitting a general classifier to use in Grid Search to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(metric = 'minkowski')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_neighbors': range(2,40),'p':[1]}] \n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(best_accuracy,end='\\n')\n",
    "print(best_parameters,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(2, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i,p=1,metric='minkowski')\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(2, 40), error, color='black', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='grey', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn with best value reported from grid search for manhatten distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 38, metric = 'minkowski', p = 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test,y_pred)\n",
    "accu_KNN_5=accuracy_score(y_test,y_pred)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n",
    "mean=accuracies.mean()\n",
    "std=accuracies.std()\n",
    "from IPython.display import Markdown, display\n",
    "bold = \"\\033[1m\"\n",
    "reset = \"\\033[0;0m\"\n",
    "\n",
    "print(bold+\"KNN with n=5\\n\\n\"+reset)\n",
    "\n",
    "print(\"The Confusion Matrix\\n\",cm,end='\\n')\n",
    "print(\"The Classification report\\n\",report,end='\\n')\n",
    "print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",accu_KNN_5*100,end='\\n')\n",
    "#after using cross validation with 10 folds\n",
    "print(\"The mean of the accuracy scores with using K-cross validation with 10 folds\\n\",mean*100,end='\\n')\n",
    "print(\"The Standard Deviation of the accuracy scores with using K-cross validation with 10 folds\\n\",std*100,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, x1, y1,cv=5,n_jobs=-1)\n",
    "train_sizes \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"KNNClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "# sizes the window for readability and displays the plot\n",
    "# shows error from 0 to 1.1\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
